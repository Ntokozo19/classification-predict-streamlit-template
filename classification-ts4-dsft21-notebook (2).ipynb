{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T10:47:00.254884Z",
     "iopub.status.busy": "2021-06-22T10:47:00.254494Z",
     "iopub.status.idle": "2021-06-22T10:47:00.992426Z",
     "shell.execute_reply": "2021-06-22T10:47:00.991485Z",
     "shell.execute_reply.started": "2021-06-22T10:47:00.254850Z"
    }
   },
   "source": [
    "<img src=\"https://explore-datascience.net/images/images_admissions2/main-logo.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T10:46:04.533083Z",
     "iopub.status.busy": "2021-06-22T10:46:04.532486Z",
     "iopub.status.idle": "2021-06-22T10:46:04.546488Z",
     "shell.execute_reply": "2021-06-22T10:46:04.545215Z",
     "shell.execute_reply.started": "2021-06-22T10:46:04.532981Z"
    }
   },
   "source": [
    "<a id=\"top\"></a>\n",
    "# Team TS4  Classification Predict Notebook\n",
    "##### Members: Ntokozo Thumre, Thato  Bogopane  <sup> </sup>\n",
    "---\n",
    "* [Notebook location]()\n",
    "* [Trello board](https://trello.com/b/bKiHUpqU/classificationts4dsft21) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Table of contents**\n",
    "1. [Introduction](#intro)\n",
    "2. [Data Collection](#data)\n",
    "3. [Data cleaning and Transformation](#cleaning)\n",
    "4. [Exploratory Data Analysis](#EDA)\n",
    "5. [Model Building](#model)\n",
    "6. [Model Parameter Tuning](#tuning)\n",
    "7. [Performance Evaluation](#evaluation)\n",
    "8. [Conclusion](#conclusion)\n",
    "9. [References](#references) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "# 1. Introduction\n",
    "**EDSA - Climate Change Belief Analysis 2021**\n",
    "\n",
    "Many companies are built around lessening one’s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product/service may be received.\n",
    "\n",
    "With this in mind, we will create a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their historical tweet data.\n",
    "\n",
    "\n",
    "## Comet\n",
    "\n",
    "Comet is a great tool for version control as it records the parameters,experiments and conditions from each of the experiements- allowing for reproducing of results, or going back to a previous version of the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://d2dfxqxblmblx4.cloudfront.net/wp-content/uploads/2013/01/18151729/twitter-logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"data\"></a>\n",
    "# 2. Data Collection\n",
    "\n",
    "Data The collection of this data was funded by a Canada Foundation for Innovation JELF Grant to Chris Bauch, University of Waterloo. The dataset aggregates tweets pertaining to climate change collected between Apr 27, 2015 and Feb 21, 2018. In total, 43943 tweets were collected. Each tweet is labelled as one of the following classes:\n",
    "\n",
    "#### Class Description \n",
    "\n",
    "2 - News: the tweet links to factual news about climate change \n",
    "\n",
    "1 - Pro: the tweet supports the belief of man-made climate change\n",
    "\n",
    "0 - Neutral: the tweet neither supports nor refutes the belief of man-made climate change \n",
    "\n",
    "-1 - Anti: the tweet does not believe in man-made climate change \n",
    "\n",
    "#### Variable definitions \n",
    "\n",
    "sentiment: Sentiment of tweet\n",
    "\n",
    "message: Tweet body \n",
    "\n",
    "tweetid: Twitter unique id files \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:58.367849Z",
     "iopub.status.busy": "2021-06-22T13:50:58.367386Z",
     "iopub.status.idle": "2021-06-22T13:50:58.403289Z",
     "shell.execute_reply": "2021-06-22T13:50:58.402293Z",
     "shell.execute_reply.started": "2021-06-22T13:50:58.367772Z"
    }
   },
   "outputs": [],
   "source": [
    "#pandas and visualizations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "%matplotlib inline\n",
    "\n",
    "# tweet cleaning and transformation\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import unidecode\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from nltk import bigrams\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from wordcloud import STOPWORDS\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# training the model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Asssessing the model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "#resampling module\n",
    "from sklearn.utils import resample\n",
    "\n",
    "#hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "#pandas and visualizations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# tweet cleaning and transformation\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "#import preprocessor as p\n",
    "import string\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize, TreebankWordTokenizer\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from wordcloud import WordCloud\n",
    "# training the model\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Asssessing the model\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "#resampling module\n",
    "from sklearn.utils import resample\n",
    "#hyperparameter tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:58.522705Z",
     "iopub.status.busy": "2021-06-22T13:50:58.522124Z",
     "iopub.status.idle": "2021-06-22T13:50:58.622330Z",
     "shell.execute_reply": "2021-06-22T13:50:58.621203Z",
     "shell.execute_reply.started": "2021-06-22T13:50:58.522672Z"
    }
   },
   "outputs": [],
   "source": [
    "#Read the data\n",
    "df_train = pd.read_csv('../input/edsa-climate-change-belief-analysis-2021/train.csv')\n",
    "df_test = pd.read_csv('../input/edsa-climate-change-belief-analysis-2021/test.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:58.656742Z",
     "iopub.status.busy": "2021-06-22T13:50:58.656166Z",
     "iopub.status.idle": "2021-06-22T13:50:58.666126Z",
     "shell.execute_reply": "2021-06-22T13:50:58.665213Z",
     "shell.execute_reply.started": "2021-06-22T13:50:58.656708Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Before we do anything to the original dataframe we will make a copy of it so that we always have the original dataframe when we need to reference it. We will then do our cleaning and preprocessing on the copy and use the original as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:58.716314Z",
     "iopub.status.busy": "2021-06-22T13:50:58.715923Z",
     "iopub.status.idle": "2021-06-22T13:50:58.721179Z",
     "shell.execute_reply": "2021-06-22T13:50:58.720368Z",
     "shell.execute_reply.started": "2021-06-22T13:50:58.716283Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all = df_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cleaning\"></a>\n",
    "# 3. Data cleaning and Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Cleaning out Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:58.786175Z",
     "iopub.status.busy": "2021-06-22T13:50:58.785818Z",
     "iopub.status.idle": "2021-06-22T13:50:58.797335Z",
     "shell.execute_reply": "2021-06-22T13:50:58.796139Z",
     "shell.execute_reply.started": "2021-06-22T13:50:58.786145Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test.isnull().sum() # checking for null values on our test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:58.899808Z",
     "iopub.status.busy": "2021-06-22T13:50:58.899393Z",
     "iopub.status.idle": "2021-06-22T13:50:58.911295Z",
     "shell.execute_reply": "2021-06-22T13:50:58.910257Z",
     "shell.execute_reply.started": "2021-06-22T13:50:58.899758Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all.isnull().sum() # checking for null values on our train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values to clean out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Sentiment cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function turns the sentiment codes into actual words which will make it easier for us to explore and visaulise the data going forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:58.953328Z",
     "iopub.status.busy": "2021-06-22T13:50:58.952938Z",
     "iopub.status.idle": "2021-06-22T13:50:58.960406Z",
     "shell.execute_reply": "2021-06-22T13:50:58.959052Z",
     "shell.execute_reply.started": "2021-06-22T13:50:58.953296Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_sentiment(df):  \n",
    "    \"\"\"This function turns the sentiment codes into actual words which will make it easier for us to see\"\"\"\n",
    "    \n",
    "    # creating a empty list for storage \n",
    "    sentiment_list = []\n",
    "    \n",
    "    # Going through each sentiment and changing them accordingly, this is what this loop and the if statements do.\n",
    "    for number in df['sentiment'] :\n",
    "        \n",
    "        if number == 1 :\n",
    "            sentiment_list.append('Pro')\n",
    "            \n",
    "        elif number == 0 :\n",
    "            sentiment_list.append('Neutral')\n",
    "            \n",
    "        elif number == -1 :\n",
    "            sentiment_list.append('Anti')\n",
    "            \n",
    "        else :\n",
    "            sentiment_list.append('News')\n",
    "            \n",
    "    # putting our sentiments in the column named 'sentiment' to our dataframe\n",
    "    df_all['sentiment'] = sentiment_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:59.065959Z",
     "iopub.status.busy": "2021-06-22T13:50:59.065552Z",
     "iopub.status.idle": "2021-06-22T13:50:59.089849Z",
     "shell.execute_reply": "2021-06-22T13:50:59.088914Z",
     "shell.execute_reply.started": "2021-06-22T13:50:59.065928Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_sentiment(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-13T20:10:18.727908Z",
     "iopub.status.busy": "2021-06-13T20:10:18.727181Z",
     "iopub.status.idle": "2021-06-13T20:10:18.732034Z",
     "shell.execute_reply": "2021-06-13T20:10:18.730603Z",
     "shell.execute_reply.started": "2021-06-13T20:10:18.727866Z"
    }
   },
   "source": [
    "### 3.3.1 Tweet cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clean our tweet data by removing all the noise, that is, removing all the urls, hashtags, mensions, puntuation and making all the text lowercase. This function will remove hashtags therefore we will start by making a copy of all the hashtags in order to analyse them seperately later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:59.177244Z",
     "iopub.status.busy": "2021-06-22T13:50:59.176853Z",
     "iopub.status.idle": "2021-06-22T13:50:59.327460Z",
     "shell.execute_reply": "2021-06-22T13:50:59.326091Z",
     "shell.execute_reply.started": "2021-06-22T13:50:59.177210Z"
    }
   },
   "outputs": [],
   "source": [
    "def hashtag_extract(tweet):  \n",
    "    \"\"\"Helper function to extract hashtags\"\"\"\n",
    "    # creating a empty list for storage where we will keep our Hashtags later\n",
    "    hashtags = []\n",
    "    \n",
    "    # Going through each tweet and looking for each hashtag and appending the Hashtags in our empty list hashtags\n",
    "    for i in tweet:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "    \n",
    "    # finding the sum of the elements in the list hashtag\n",
    "    hashtags = sum(hashtags, [])\n",
    "    \n",
    "    # creating a dictionary with tokens from the list hashtags into a dictionary, where the keys are the frequency and the values is the frequency\n",
    "    frequency = nltk.FreqDist(hashtags)\n",
    "    \n",
    "    # creating a dataframe from the dictionary to keep track of the word and the frequency\n",
    "    hashtag_df = pd.DataFrame({'hashtag': list(frequency.keys()),\n",
    "                           'count': list(frequency.values())})\n",
    "    \n",
    "    # method is used to get n largest values from a dataframe \n",
    "    hashtag_df = hashtag_df.nlargest(15, columns=\"count\")\n",
    "\n",
    "    return hashtag_df\n",
    "\n",
    "\n",
    "#Extracting the hashtags for the pro sentiment tweets \n",
    "pro = hashtag_extract(df_all['message'][df_all['sentiment'] == 'Pro'])\n",
    "\n",
    "#Extracting the hashtags for the Anti sentiment tweets\n",
    "anti = hashtag_extract(df_all['message'][df_all['sentiment'] == 'Anti'])\n",
    "\n",
    "#Extracting the hashtags for the Neutral sentiment tweets\n",
    "neutral = hashtag_extract(df_all['message'][df_all['sentiment'] == 'Neutral'])\n",
    "\n",
    "#Extracting the hashtags for the News sentiment tweets\n",
    "news = hashtag_extract(df_all['message'][df_all['sentiment'] == \"News\"])\n",
    "\n",
    "\n",
    "#creating a dataframe with all the hashtags and a count for each sentiment\n",
    "df_hashtags = pro.merge(anti,on='hashtag',suffixes=('_pro', '_anti'), how = 'outer').merge(neutral,on='hashtag', how = 'outer').merge(news,on='hashtag', suffixes = ('_neutral', '_news'), how = 'outer')\n",
    "df_hashtags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this table later in our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:59.329559Z",
     "iopub.status.busy": "2021-06-22T13:50:59.329223Z",
     "iopub.status.idle": "2021-06-22T13:50:59.391760Z",
     "shell.execute_reply": "2021-06-22T13:50:59.390773Z",
     "shell.execute_reply.started": "2021-06-22T13:50:59.329526Z"
    }
   },
   "outputs": [],
   "source": [
    "# Separate joined words based on capitals\n",
    "def camel_case_split(identifier):\n",
    "    \n",
    "    matches = re.finditer(\n",
    "        r'.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)',\n",
    "        identifier\n",
    "    )\n",
    "    return \" \".join([m.group(0) for m in matches])\n",
    "\n",
    "# Extract Mentions\n",
    "def extract_mentions(tweet):\n",
    "    \n",
    "  \"\"\"Helper function to extract mentions\"\"\"\n",
    "  mentions = re.findall(r'@([a-zA-Z0-9_]{1}[a-zA-Z0-9_]{0,14})', tweet)\n",
    "    \n",
    "  return mentions\n",
    "\n",
    "#Applying the function on the dataframe\n",
    "df_all['mentions'] = df_all['message'].apply(extract_mentions)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Putting the mentions in their respective list so we can plot some graphs later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:59.393787Z",
     "iopub.status.busy": "2021-06-22T13:50:59.393456Z",
     "iopub.status.idle": "2021-06-22T13:50:59.435146Z",
     "shell.execute_reply": "2021-06-22T13:50:59.434095Z",
     "shell.execute_reply.started": "2021-06-22T13:50:59.393755Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of all the mentions\n",
    "mentions_list = [item for new_list in df_all['mentions'] for item in new_list]\n",
    "\n",
    "# Grouping mentions by sentiment\n",
    "# News Mentions\n",
    "news_mentions = df_all[df_all['sentiment'] == 'News']['mentions']\n",
    "news_mentions = [x for x in news_mentions if x != []]\n",
    "news_mentions = [item for new_list in news_mentions for item in new_list]\n",
    "\n",
    "# Positive Mentions\n",
    "pos_mentions = df_all[df_all['sentiment'] == 'Pro']['mentions']\n",
    "pos_mentions = [x for x in pos_mentions if x != []]\n",
    "pos_mentions = [item for new_list in pos_mentions for item in new_list]\n",
    "\n",
    "# Neutral Mentions\n",
    "neutral_mentions =df_all[df_all['sentiment'] == 'Neutral']['mentions']\n",
    "neutral_mentions = [x for x in neutral_mentions if x != []]\n",
    "neutral_mentions = [item for new_list in neutral_mentions for item in new_list]\n",
    "\n",
    "# Negative Mentions\n",
    "neg_mentions = df_all[df_all['sentiment'] == 'Anti']['mentions']\n",
    "neg_mentions = [x for x in neg_mentions if x != []]\n",
    "neg_mentions = [item for new_list in neg_mentions for item in new_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Cleaning the tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to the english language, we use a lot of contractions (e.g it's instead of it is) which are deteremined by removing some letters and replacing them with an apostrophe. During our cleaning process we will be removing the puntuation which will also remove the apostrophe's in contractions therefore changinfg the meaning of these words. So, we will creat a function that will look up the well known contractions and replace them with the full word. This will also assist when we apply ngram.\n",
    "When it comes to social media, people also use popular short hand words in place of the full words e.g (lol instead of laugh out load or idk instead of I dont know). We want our model to be able to capture these words and their true meaning instead of captureing random letters.\n",
    "So, we will add the well known short hand words in our dictionary aswell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:59.436987Z",
     "iopub.status.busy": "2021-06-22T13:50:59.436637Z",
     "iopub.status.idle": "2021-06-22T13:50:59.548451Z",
     "shell.execute_reply": "2021-06-22T13:50:59.547235Z",
     "shell.execute_reply.started": "2021-06-22T13:50:59.436954Z"
    }
   },
   "outputs": [],
   "source": [
    "#URLs,Hashtags,Mentions,Reserved words (RT, FAV),Emojis,Smileys and make everything lowercase\n",
    "\n",
    "#Removing RT ftom tweets\n",
    "df_all['message'] = df_all['message'].str.strip('rt ')\n",
    "\n",
    "df_test['message'] = df_test['message'].str.strip('rt ')\n",
    "\n",
    "# Remove @ mentions\n",
    "pattern = r\"@[\\w]+\" # pattern to remove\n",
    "\n",
    "pattern = r\"@[\\w]+\" # pattern to remove\n",
    "\n",
    "sub = r'' # replace it with with an empty space \n",
    "\n",
    "#transforming our dataframe \n",
    "df_all['message'] = df_all['message'].replace(to_replace = pattern, value = sub, regex = True)\n",
    "\n",
    "#transforming our dataframe \n",
    "df_test['message'] = df_test['message'].replace(to_replace = pattern, value = sub, regex = True) \n",
    "\n",
    "df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:59.550349Z",
     "iopub.status.busy": "2021-06-22T13:50:59.549998Z",
     "iopub.status.idle": "2021-06-22T13:50:59.563579Z",
     "shell.execute_reply": "2021-06-22T13:50:59.562518Z",
     "shell.execute_reply.started": "2021-06-22T13:50:59.550317Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:59.608255Z",
     "iopub.status.busy": "2021-06-22T13:50:59.607855Z",
     "iopub.status.idle": "2021-06-22T13:50:59.614219Z",
     "shell.execute_reply": "2021-06-22T13:50:59.612893Z",
     "shell.execute_reply.started": "2021-06-22T13:50:59.608225Z"
    }
   },
   "outputs": [],
   "source": [
    "#Function used to lookup shortwords from the dictionary\n",
    "def lookup_dict(text, dictionary):\n",
    "    \n",
    "    for word in text.split(): \n",
    "        \n",
    "        if word.lower() in dictionary:\n",
    "            \n",
    "            if word.lower() in text.split():\n",
    "                \n",
    "                text = text.replace(word, dictionary[word.lower()]) \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:59.685605Z",
     "iopub.status.busy": "2021-06-22T13:50:59.685028Z",
     "iopub.status.idle": "2021-06-22T13:50:59.886647Z",
     "shell.execute_reply": "2021-06-22T13:50:59.885573Z",
     "shell.execute_reply.started": "2021-06-22T13:50:59.685555Z"
    }
   },
   "outputs": [],
   "source": [
    "#create a dictionary of contractions as the keys and its full word representation as the values\n",
    "short_and_contractions = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\",\n",
    "\"121\": \"one to one\",\n",
    "\"a/s/l\": \"age, sex, location\",\n",
    "\"adn\": \"any day now\",\n",
    "\"afaik\": \"as far as I know\",\n",
    "\"afk\": \"away from keyboard\",\n",
    "\"aight\": \"alright\",\n",
    "\"alol\": \"actually laughing out loud\",\n",
    "\"b4\": \"before\",\n",
    "\"b4n\": \"bye for now\",\n",
    "\"bak\": \"back at the keyboard\",\n",
    "\"bf\": \"boyfriend\",\n",
    "\"bff\": \"best friends forever\",\n",
    "\"bfn\": \"bye for now\",\n",
    "\"bg\": \"big grin\",\n",
    "\"bta\": \"but then again\",\n",
    "\"btw\": \"by the way\",\n",
    "\"cid\": \"crying in disgrace\",\n",
    "\"cnp\": \"continued in my next post\",\n",
    "\"cp\": \"chat post\",\n",
    "\"cu\": \"see you\",\n",
    "\"cul\": \"see you later\",\n",
    "\"cul8r\": \"see you later\",\n",
    "\"cya\": \"bye\",\n",
    "\"cyo\": \"see you online\",\n",
    "\"dbau\": \"doing business as usual\",\n",
    "\"fud\": \"fear, uncertainty, and doubt\",\n",
    "\"fwiw\": \"for what it's worth\",\n",
    "\"fyi\": \"for your information\",\n",
    "\"g\": \"grin\",\n",
    "\"g2g\": \"got to go\",\n",
    "\"ga\": \"go ahead\",\n",
    "\"gal\": \"get a life\",\n",
    "\"gf\": \"girlfriend\",\n",
    "\"gfn\": \"gone for now\",\n",
    "\"gmbo\": \"giggling my butt off\",\n",
    "\"gmta\": \"great minds think alike\",\n",
    "\"h8\": \"hate\",\n",
    "\"hagn\": \"have a good night\",\n",
    "\"hdop\": \"help delete online predators\",\n",
    "\"hhis\": \"hanging head in shame\",\n",
    "\"iac\": \"in any case\",\n",
    "\"ianal\": \"I am not a lawyer\",\n",
    "\"ic\": \"I see\",\n",
    "\"idk\": \"I don't know\",\n",
    "\"imao\": \"in my arrogant opinion\",\n",
    "\"imnsho\": \"in my not so humble opinion\",\n",
    "\"imo\": \"in my opinion\",\n",
    "\"iow\": \"in other words\",\n",
    "\"ipn\": \"I’m posting naked\",\n",
    "\"irl\": \"in real life\",\n",
    "\"jk\": \"just kidding\",\n",
    "\"l8r\": \"later\",\n",
    "\"ld\": \"later, dude\",\n",
    "\"ldr\": \"long distance relationship\",\n",
    "\"llta\": \"lots and lots of thunderous applause\",\n",
    "\"lmao\": \"laugh my ass off\",\n",
    "\"lmirl\": \"let's meet in real life\",\n",
    "\"lol\": \"laugh out loud\",\n",
    "\"ltr\": \"longterm relationship\",\n",
    "\"lulab\": \"love you like a brother\",\n",
    "\"lulas\": \"love you like a sister\",\n",
    "\"luv\": \"love\",\n",
    "\"m/f\": \"male or female\",\n",
    "\"m8\": \"mate\",\n",
    "\"milf\": \"mother I would like to fuck\",\n",
    "\"oll\": \"online love\",\n",
    "\"omg\": \"oh my god\",\n",
    "\"otoh\": \"on the other hand\",\n",
    "\"pir\": \"parent in room\",\n",
    "\"ppl\": \"people\",\n",
    "\"r\": \"are\",\n",
    "\"rofl\": \"roll on the floor laughing\",\n",
    "\"rpg\": \"role playing games\",\n",
    "\"ru\": \"are you\",\n",
    "\"shid\": \"slaps head in disgust\",\n",
    "\"somy\": \"sick of me yet\",\n",
    "\"sot\": \"short of time\",\n",
    "\"thanx\": \"thanks\",\n",
    "\"thx\": \"thanks\",\n",
    "\"ttyl\": \"talk to you later\",\n",
    "\"u\": \"you\",\n",
    "\"ur\": \"you are\",\n",
    "\"uw\": \"you’re welcome\",\n",
    "\"wb\": \"welcome back\",\n",
    "\"wfm\": \"works for me\",\n",
    "\"wibni\": \"wouldn't it be nice if\",\n",
    "\"wtf\": \"what the fuck\",\n",
    "\"wtg\": \"way to go\",\n",
    "\"wtgp\": \"want to go private\",\n",
    "\"ym\": \"young man\",\n",
    "\"gr8\": \"great\",\n",
    "\"8yo\":\"eight year old\",\n",
    "\"brb\" : \"be right back\"    \n",
    "}\n",
    "\n",
    "#Apply a lambda function to look up every word in the tweets and replace it with the full word\n",
    "\n",
    "#apply this to a new column so that we can see the difference\n",
    "df_all['clean_message'] = df_all['message'].apply(lambda x: lookup_dict(x,short_and_contractions))\n",
    "df_test['clean_message'] = df_test['message'].apply(lambda x: lookup_dict(x,short_and_contractions))\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:59.888533Z",
     "iopub.status.busy": "2021-06-22T13:50:59.888248Z",
     "iopub.status.idle": "2021-06-22T13:50:59.903691Z",
     "shell.execute_reply": "2021-06-22T13:50:59.902727Z",
     "shell.execute_reply.started": "2021-06-22T13:50:59.888504Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the word doesn't has been changed to does not therefore this word will not be afftected by our 'remove puntuation' steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:59.905758Z",
     "iopub.status.busy": "2021-06-22T13:50:59.905382Z",
     "iopub.status.idle": "2021-06-22T13:50:59.916791Z",
     "shell.execute_reply": "2021-06-22T13:50:59.915576Z",
     "shell.execute_reply.started": "2021-06-22T13:50:59.905714Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"This function is to strip accent letter those special characters then return a normal alphabet\"\"\"\n",
    "def strip_accents(text):\n",
    "\n",
    "    try:\n",
    "        text = unicode(text, 'utf-8')\n",
    "    except NameError: # unicode is a default on python 3\n",
    "        pass\n",
    "\n",
    "    text = unicodedata.normalize('NFD', text)\\\n",
    "           .encode('ascii', 'ignore')\\\n",
    "           .decode(\"utf-8\")\n",
    "\n",
    "    return str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:50:59.946957Z",
     "iopub.status.busy": "2021-06-22T13:50:59.946434Z",
     "iopub.status.idle": "2021-06-22T13:51:00.028510Z",
     "shell.execute_reply": "2021-06-22T13:51:00.027650Z",
     "shell.execute_reply.started": "2021-06-22T13:50:59.946916Z"
    }
   },
   "outputs": [],
   "source": [
    "# applying the function on our training data\n",
    "df_all['clean_message'] = df_all['clean_message'].apply(strip_accents) \n",
    "\n",
    "# applying the function on our testing data\n",
    "df_test['clean_message'] = df_test['clean_message'].apply(strip_accents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:00.049056Z",
     "iopub.status.busy": "2021-06-22T13:51:00.048481Z",
     "iopub.status.idle": "2021-06-22T13:51:03.511541Z",
     "shell.execute_reply": "2021-06-22T13:51:03.510310Z",
     "shell.execute_reply.started": "2021-06-22T13:51:00.049005Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"This function is to clean the data removing urls, punctuations, spaces and making text to be lowercase \"\"\"\n",
    "    \n",
    "    URL = r'http[s]?://(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n",
    "    text = re.sub(URL, '', text)\n",
    "    \n",
    "    text = text.lower() # making text to be lowercase \n",
    "      \n",
    "    text =re.sub(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])|(\\%)|(\\$)|(\\>)|(\\<)|(\\{)|(\\})\", \"\", text) # removing all punctuation with no space\n",
    "    \n",
    "    text =re.sub(\"(<br\\s/><br\\s/?)|(-)|(_)|(/)|(:).\", \" \", text) # removing all punctuation with a space\n",
    "    \n",
    "    text = re.sub(\"\\\\s+\", \" \", text)  # Remove extra whitespace\n",
    "       \n",
    "    text = re.sub(r\"U+FFFD \", ' ', text) # Remove that funny diamond\n",
    "    \n",
    "    text = text.lstrip()  # removes whitespaces before string\n",
    "    \n",
    "    text = text.rstrip()  # removes whitespaces after string\n",
    "    \n",
    "    return text\n",
    "\n",
    "df_all['clean_message'] = df_all['clean_message'].apply(clean_text)\n",
    "\n",
    "df_test['clean_message'] = df_test['clean_message'].apply(clean_text)\n",
    "\n",
    "df_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:03.513859Z",
     "iopub.status.busy": "2021-06-22T13:51:03.513491Z",
     "iopub.status.idle": "2021-06-22T13:51:03.530956Z",
     "shell.execute_reply": "2021-06-22T13:51:03.529818Z",
     "shell.execute_reply.started": "2021-06-22T13:51:03.513824Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second round cleaning since we saw some of the characters were not removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:03.534021Z",
     "iopub.status.busy": "2021-06-22T13:51:03.533606Z",
     "iopub.status.idle": "2021-06-22T13:51:03.909207Z",
     "shell.execute_reply": "2021-06-22T13:51:03.908185Z",
     "shell.execute_reply.started": "2021-06-22T13:51:03.533985Z"
    }
   },
   "outputs": [],
   "source": [
    "# removing punctuations on our training dataset\n",
    "df_all['clean_message'] = df_all['clean_message'].apply(lambda x: ''.join([l for l in x if l not in string.punctuation]))\n",
    "\n",
    "# removing punctuations on our testing dataset\n",
    "df_test['clean_message'] = df_test['clean_message'].apply(lambda x: ''.join([l for l in x if l not in string.punctuation]))\n",
    "\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:03.911310Z",
     "iopub.status.busy": "2021-06-22T13:51:03.910943Z",
     "iopub.status.idle": "2021-06-22T13:51:03.931369Z",
     "shell.execute_reply": "2021-06-22T13:51:03.929928Z",
     "shell.execute_reply.started": "2021-06-22T13:51:03.911278Z"
    }
   },
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Transform the Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now tokenise our tweets, tag them to their parts of speech and lemmetize them.\n",
    "\n",
    "\n",
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:03.933546Z",
     "iopub.status.busy": "2021-06-22T13:51:03.933103Z",
     "iopub.status.idle": "2021-06-22T13:51:06.556844Z",
     "shell.execute_reply": "2021-06-22T13:51:06.555831Z",
     "shell.execute_reply.started": "2021-06-22T13:51:03.933500Z"
    }
   },
   "outputs": [],
   "source": [
    "#start by tokenizing the tweets\n",
    "tokeniser = TreebankWordTokenizer()\n",
    "\n",
    "# transforming the data using the Treebankword tokenizer\n",
    "df_all['tokenized'] = df_all['clean_message'].apply(tokeniser.tokenize)\n",
    "\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parts of Speech tagging\n",
    "\n",
    "Part of Speech Tags are useful for building parse trees, which are used in building named entity recognition (most named entities are Nouns) and extracting relations between words. POS Tagging is also essential for building lemmatizers which are used to reduce a word to its root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:06.558381Z",
     "iopub.status.busy": "2021-06-22T13:51:06.558086Z",
     "iopub.status.idle": "2021-06-22T13:51:26.393426Z",
     "shell.execute_reply": "2021-06-22T13:51:26.392338Z",
     "shell.execute_reply.started": "2021-06-22T13:51:06.558351Z"
    }
   },
   "outputs": [],
   "source": [
    "# converting the column clean_message in the dataframe into a list\n",
    "texts = df_all['clean_message'].tolist()\n",
    "\n",
    "# tagging the tweets\n",
    "tagged_texts = pos_tag_sents(map(word_tokenize, texts))\n",
    "df_all['POS'] = tagged_texts\n",
    "\n",
    "\"\"\" this function indentifies the parts of speech Noun, Adjective, Verb and Adverb \"\"\"\n",
    "def get_wordnet_pos(tag):\n",
    "    \n",
    "    \n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    \n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:26.394970Z",
     "iopub.status.busy": "2021-06-22T13:51:26.394624Z",
     "iopub.status.idle": "2021-06-22T13:51:26.668973Z",
     "shell.execute_reply": "2021-06-22T13:51:26.667677Z",
     "shell.execute_reply.started": "2021-06-22T13:51:26.394938Z"
    }
   },
   "outputs": [],
   "source": [
    "# transforming the data with the function we created\n",
    "df_all['POS'] = df_all['POS'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "df_all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:26.672261Z",
     "iopub.status.busy": "2021-06-22T13:51:26.671878Z",
     "iopub.status.idle": "2021-06-22T13:51:28.171414Z",
     "shell.execute_reply": "2021-06-22T13:51:28.170260Z",
     "shell.execute_reply.started": "2021-06-22T13:51:26.672225Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialising our lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# applying lemmatization to the POS column\n",
    "df_all['lemmatized'] = df_all['POS'].apply(lambda x: [lemmatizer.lemmatize(word, tag) for word, tag in x])\n",
    "\n",
    "# joining the tokenised words after they have been lemmatized\n",
    "df_all['lemmatized'] = [' '.join(map(str, l)) for l in df_all['lemmatized']]\n",
    "\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"EDA\"></a>\n",
    "# 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Sentiment Distribution\n",
    "We can get a lot of insight from our data by exploring the distibution of our sentiments in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:28.173492Z",
     "iopub.status.busy": "2021-06-22T13:51:28.173212Z",
     "iopub.status.idle": "2021-06-22T13:51:28.323953Z",
     "shell.execute_reply": "2021-06-22T13:51:28.322866Z",
     "shell.execute_reply.started": "2021-06-22T13:51:28.173464Z"
    }
   },
   "outputs": [],
   "source": [
    "# creating a list for the names of each section of the pie\n",
    "labels=['Pro', 'News', 'Neutral', 'Anti'] \n",
    "\n",
    "plt.pie(df_all['sentiment'].value_counts(),\n",
    "            labels=labels,\n",
    "            autopct='%1.0f%%',\n",
    "            shadow=True,\n",
    "            startangle=90,\n",
    "            explode = (0.1, 0.1, 0.1, 0.1), radius = 2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the majority of our data (54%) lies in the pro climate change group where only 8% of our data represents the tweets that are anti climate change. 15% percent of our tweets represent people that are neutral and and 23% of the tweets are news tweets about climate change."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Tweet length distribution\n",
    "\n",
    "It would also be interesting to look at the length of the tweets for each sentiment, which sentiment writes the longest or the shortest tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:28.325699Z",
     "iopub.status.busy": "2021-06-22T13:51:28.325371Z",
     "iopub.status.idle": "2021-06-22T13:51:28.362030Z",
     "shell.execute_reply": "2021-06-22T13:51:28.360899Z",
     "shell.execute_reply.started": "2021-06-22T13:51:28.325660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finding Number of Words per Tweet from the lemmatized words\n",
    "df_all[\"num_words\"] = df_all[\"lemmatized\"].apply(lambda x: len(str(x).split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:28.364103Z",
     "iopub.status.busy": "2021-06-22T13:51:28.363642Z",
     "iopub.status.idle": "2021-06-22T13:51:28.625814Z",
     "shell.execute_reply": "2021-06-22T13:51:28.624822Z",
     "shell.execute_reply.started": "2021-06-22T13:51:28.364064Z"
    }
   },
   "outputs": [],
   "source": [
    "fig_dims = (10, 6)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "\n",
    "# plotting boxplot for the length of how long each tweet is per sentiment \n",
    "sns.boxplot(x=df_all['sentiment'], y=df_all['lemmatized'].str.len(), data=df_all, palette=(\"rainbow\"), ax=ax)\n",
    "\n",
    "# title of the boxplot\n",
    "plt.title('Length of characters in each tweet per sentiment')\n",
    "\n",
    "# showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that the tweets that represent the pro climate change group are generally longer than the other sentiments, meaning people that are pro climate change group write longer tweets as compared to the other groups. We can also see that people who are against climate change generally write shorter tweets as compared to the pro and neutral groups and this might suggest that they are only stating opnions with no evidence, or expressing how they feel at that moment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:28.627440Z",
     "iopub.status.busy": "2021-06-22T13:51:28.627115Z",
     "iopub.status.idle": "2021-06-22T13:51:28.859917Z",
     "shell.execute_reply": "2021-06-22T13:51:28.858861Z",
     "shell.execute_reply.started": "2021-06-22T13:51:28.627407Z"
    }
   },
   "outputs": [],
   "source": [
    "fig_dims = (10, 6)\n",
    "fig, ax = plt.subplots(figsize=fig_dims)\n",
    "\n",
    "# plotting boxplot for the length of how long each tweet is per sentiment \n",
    "sns.boxplot(x='sentiment', y='num_words', data=df_all, palette=(\"hls\"))\n",
    "\n",
    "# title of the boxplot\n",
    "plt.title('Number of words in each tweet for each sentiment')\n",
    "\n",
    "# showing the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that the tweets that represent the pro climate change group are generally longer than the other sentiments, as we expected that since they had more characters but also the Neutral climate change group has a bigger distribution than any group meaning most people in this group can have type any number of words which makes it difficult differentiate the groups based on the number of words per tweet and we know that in English each word is made out of 5 letters on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Hashtag analysis\n",
    "Now lets look at the hashtags that were used in the tweets. This will give us an indication of which hashtags each sentiment frequently uses. This could possibly help us tell whether a tweet is Pro or Anti climate change.\n",
    "\n",
    "We previously extracted the hashtags before cleaning the 'message column', we will now use that dataframe of hashtags in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:28.861666Z",
     "iopub.status.busy": "2021-06-22T13:51:28.861338Z",
     "iopub.status.idle": "2021-06-22T13:51:30.476751Z",
     "shell.execute_reply": "2021-06-22T13:51:30.475639Z",
     "shell.execute_reply.started": "2021-06-22T13:51:28.861632Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, \n",
    "                         nrows=2, \n",
    "                         figsize=(20, 15))\n",
    "\n",
    "sns.barplot(data=pro,y=pro['hashtag'], x=pro['count'], ax=axes[0,0]).set(title = 'Pro climate change hashtags')\n",
    "\n",
    "sns.barplot(data=anti,y=anti['hashtag'], x=anti['count'], ax=axes[0,1]).set(title = 'Anti climate change hashtags')\n",
    "\n",
    "sns.barplot(data=neutral,y=neutral['hashtag'], x=neutral['count'], ax=axes[1,0]).set(title = 'Neutral climate change hashtags')\n",
    "\n",
    "sns.barplot(data=news,y=news['hashtag'], x=news['count'], ax=axes[1,1]).set(title = 'News climate change hashtags')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insights:\n",
    "\n",
    "_**Pro climate change hashtags:**_\n",
    "\n",
    "\n",
    "* The most used pro climate change hashtag outside of the #climate is #beforetheflood. This hashtag comes from the documentary named Before the flood where the famous actor Leonardo DiCaprio meets with scientists, activists and world leaders to discuss the dangers of climate change and possible solutions. This documentary created a lot of awareness and education around climate change and the causes of it. It brought, to everyday people's TV screens, the truth about climate change and the true impact it has and will continue to have on the planet if we dont change our way of living. Since this show, many people have jumped on the pro climate change band wagon and have started speaking out, which is what we are seeing with this hashtag.\n",
    "\n",
    "\n",
    "* Another famous hashtag in the pro climate change hashtags is the #ImVotingBecause. As Americans select their next president, voters share their thoughts as to why they have chosen their candidate with the hashtag #ImVotingBecause. The social media landscape shows that supporters are firm in their convictions and consider this election a historic one. This hashtag also falls under climate change issues because in America climate change has become one of the most important issues in politics and climate change supporters want to vote for a president who views climate change as a priority. Donald Trump was also a president who reversed a lot of work done by climate change advocates around the world.\n",
    "\n",
    "\n",
    "* Another hashtag that was in the top 5 was the #COP22 which is The 2016 United Nations Climate Change Conference that was an international meeting of political leaders and activists to discuss environmental issues. It was held in Marrakech, Morocco, on 7–18 November 2016. This is where the Trump administration formerly announced their plans to exit the climate change deal. Climate change supporters where not happy with this decision. This also exlains the #Trump which also appears at number 9 of the pro climate change hashshtags.\n",
    "\n",
    "\n",
    "_**Anti climate change hashtags:**_\n",
    "\n",
    "* The number hastag used by anti climate changers is #MAGA. \"Make America Great Again\" or MAGA is a campaign slogan used in American politics popularized by Donald Trump in his successful 2016 presidential campaign. During his presidency Donald trump did not shy away from expressing his anti climate change views and his supporters didnt either. That is why #Trump is also the 3rd most frequently used hashtag in the anti climate change hashtags.\n",
    "\n",
    "\n",
    "* Hashtags such as #Fakenews, #climatescam, #DrainTheSwamp also made it to the top hashtags and they are all related to anti climate chmage supporters who believe thet climate change is a lie or a scam which is also perpurtuated by Donald Trump. So we can see from all these hashtags that Donald Trump is the biggest leader who is anti climate change and who's supporters are also anti climate change.\n",
    "\n",
    "\n",
    "* TCOT which stands for Top Conservative On Twitter takes the number 8 spot. The term provides a way for conservatives in particular and Republicans in general to locate and follow the tweets of like-minded individuals. We're sensing a pattern here: Trump, top conservatives on twitter, make America great again, Drain the swamp\n",
    "\n",
    "\n",
    "\n",
    "_**Neutral climate change hashtags:**_\n",
    "\n",
    "* In the neutral category, a few of the top hashtags were not directly related to climate change eg. Hashtags like GlobalWarming, AmReading, budget and QandA could suggest that these tweets are made by people who are still undecided on the topic but could be open to discussions and are interested in finding new/more information.\n",
    "\n",
    "* In general neutral climate change tweets have hashtags that are not as polarized as the anti and pro hashtags.\n",
    "\n",
    "* America and China are responsible for 40% of the world's carbon emissions and are the most mentioned geopolitical entities in pro climate change tweets, most likely for this reason\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_**News climate change hashtags:**_\n",
    "\n",
    "* The hashtags in the news category are less emotive and aim to bring awareness to high profile topics related to climate change that are or were trending in the news. Examples of such hashtags include News and Science which would be used to indicate that the tweet contains information from a news outlet or a scientific study.\n",
    "\n",
    "* ParisAgreement, COP22 and Trump are popular hashtags. Trump made headlines when he pulled out of the climate agreement, so it makes sense that these hashtags would be trending in climate change news.\n",
    "\n",
    "* ClimateMarch - many protests have been held, some even global, to raise awareness about climate change. These protests usually make headlines and are featured on news sites\n",
    "\n",
    "A natural next step to take is to look at what other words are used in the tweets for each that can give us an indication of the sentiment. We will build word clouds to visualize these.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Frequent words Analysis in each sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:30.478240Z",
     "iopub.status.busy": "2021-06-22T13:51:30.477953Z",
     "iopub.status.idle": "2021-06-22T13:51:35.668404Z",
     "shell.execute_reply": "2021-06-22T13:51:35.667337Z",
     "shell.execute_reply.started": "2021-06-22T13:51:30.478213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting word clouds\n",
    "news = df_all[df_all['sentiment'] == 'News']['message']\n",
    "pro = df_all[df_all['sentiment'] == 'Pro']['message']\n",
    "neutral =df_all[df_all['sentiment'] == 'Neutral']['message']\n",
    "anti = df_all[df_all['sentiment'] == 'Anti']['message']\n",
    "\n",
    "\n",
    "news = [word for line in news for word in line.split()]\n",
    "pro = [word for line in pro for word in line.split()]\n",
    "neutral = [word for line in neutral for word in line.split()]\n",
    "anti= [word for line in anti for word in line.split()]\n",
    "\n",
    "news = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=100,\n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(news))\n",
    "\n",
    "pro = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=100,\n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(pro))\n",
    "\n",
    "\n",
    "\n",
    "neutral = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=100,\n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(neutral))\n",
    "\n",
    "\n",
    "anti = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=100,\n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(anti))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize = (20, 12))\n",
    "fig.tight_layout(pad = 0)\n",
    "\n",
    "axs[0, 0].imshow(news)\n",
    "axs[0, 0].set_title('Frequent words from news climate tweets', fontsize = 20)\n",
    "axs[0, 0].axis('off')\n",
    "\n",
    "axs[0, 1].imshow(pro)\n",
    "axs[0, 1].set_title('Frequent words from pro climate tweets', fontsize = 20)\n",
    "axs[0, 1].axis('off')\n",
    "\n",
    "\n",
    "axs[1, 0].imshow(anti)\n",
    "axs[1, 0].set_title('Frequent words from anti climate tweets', fontsize = 20)\n",
    "axs[1, 0].axis('off')\n",
    "\n",
    "axs[1, 1].imshow(neutral)\n",
    "axs[1, 1].set_title('Frequent words from neutral climate tweets', fontsize = 20)\n",
    "axs[1, 1].axis('off')\n",
    "\n",
    "plt.savefig('joint_cloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above word clouds we can see that the words climate change and global warming are the most frequest words across all sentiments which is to be expected. Outside of those words we see the following:\n",
    "\n",
    "\n",
    "\n",
    "_**Frequent tweets under pro climate change:**_\n",
    " \n",
    "* We see words like believe, combat, fight, real andaction which represent the belief of the pro climate change supporters who belive that climate change is real and that action needs to be taken to stop it. \n",
    "* We also see people being mentioned - SenSenders i.e Senitor Senders who is also a public supporter of the climate change movement. We also see Trump as one of the words probably because he was publicly against climate change.\n",
    "* We also saw that people tweeted about the Environmental Protection Agency(EPA)\n",
    "* They also like using words like 'know', 'believe', 'think' because they support the belief of man-made climate.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_**Frequent tweets under News climate change:**_\n",
    "\n",
    "* Scott Morrison the senior minister who went to Sky news and showed how he is in fact a climate change denier and he doesn't do what he says to the public so a lot of people are tweeting about him as he was also discussing how to reposition climate policies.\n",
    "* We also see that people tweeted about the Environmental Protection Agency(EPA), possible the news wanted to hear the views from the representives from the EPA and We also see Trump as one of the words probably because he was publicly against climate change.\n",
    "* They also like using words like 'News', 'Study', 'scientists', 'Report', 'Energy', 'record'.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_**Frequent tweets under Neutral climate change:**_\n",
    "* We also see Trump as one of the words probably because he was publicly against climate change, we also see Leonardo DiCaprio from the documentary Before the flood.\n",
    "* They also mention a lot about The Amp which is a committee on Climate Change and they want the society to change the perceptions of climate change,and alert people about trusted sources of information, communication methods relating to decarbonising homes, carbon conscious behaviour.\n",
    "* They also like using words like 'stop', 'need', 'care', 'right', 'cause'.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_**Frequent tweets under Anti climate change:**_\n",
    "* Mostly like tweeting the word chinese world they saying the world is ran by China and they  believe Chinese control the climate, they always blame someone for it and they believe it is all made so they can mak more money we can see  Barack Obama and Donald Trump being mentioned as they were the presidents of USA.\n",
    "* They also blame it all to the scientists and  \n",
    "* They also like using words like 'fake', 'money', 'tax', 'chinese world', 'scam', 'hoax'.\n",
    "\n",
    "\n",
    "**Overall insights across all wordclouds**\n",
    "\n",
    "* The most buzzwords across all sentiments are climate change, global warming and rt (retweet). The frequency of rt ( Retweet ) means that a lot of the same information and/or opinions are being shared and viewed by large audiences. This is true for all 4 classes\n",
    "\n",
    "* 'Trump' is a frequently occuring word in all 4 classes. This is unsurprising given his controversial view on the topic and also Barack Obama is mentioned as he once said 'Climate Change Greatest Threat to Future Generations' so we can see him being mentioned a lot by the Anti group they probably saying he is the cause of this as this group will find anyone to blame for anything.\n",
    "\n",
    "* Words like real, believe, think, fight, etc. occur frequently in pro climate change tweets. In contrast, anti climate change tweets contain words such as 'hoax', 'scam', 'tax', 'liberal' and 'fake'. There is a stark difference in tone and use of emotive language in these 2 sets of tweets. From this data we could reason that people who are anti climate change believe that global warming is a 'hoax' and feel negatively towards a tax–based approach to slowing global climate change.\n",
    "\n",
    "* words like 'science' and 'scientist' occur frequently as well which could imply that people are tweeting about scientific studies that support their views on climate change.\n",
    "\n",
    "* EPA, the United States Environmental Protection Agency is another climate change 'buzzword' that appears frequently across classes.\n",
    "\n",
    "* https occurs frequently in pro climate change tweets, implying that many links are being shared around the topic of climate change. These could be links to petitions, websites and/or articles related to climate change. Interesting to note: https only occurs in the top 25 words for the pro climate change class. We seeing more links in the news class as they are sharing articles and sources of information than any other group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 Frequent words Analysis for all the sentiments combined\n",
    "\n",
    "We created a visualization of the top 15 words being used in the overall tweets using a bar graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:35.670059Z",
     "iopub.status.busy": "2021-06-22T13:51:35.669711Z",
     "iopub.status.idle": "2021-06-22T13:51:36.826789Z",
     "shell.execute_reply": "2021-06-22T13:51:36.825655Z",
     "shell.execute_reply.started": "2021-06-22T13:51:35.670028Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing words that has no relevance to the context (https, RT, CO)\n",
    "df_all['word_cloud'] = df_all['lemmatized'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "\n",
    "# Removing common words which appear in all sentiments\n",
    "remove_words = ['climate', 'change', 'rt', 'global', 'warming', 'donald', 'trump','amp', 'realDonaldTrump', 's','aaa']\n",
    "\n",
    "# Function to remove common words listed above\n",
    "def remove_common_words(message):\n",
    "  pattern = re.compile(r'\\b(' + r'|'.join(remove_words) + r')\\b\\s*')\n",
    "  message = pattern.sub('', message)\n",
    "  return message\n",
    "\n",
    "df_all['word_cloud'] = df_all['word_cloud'].apply(remove_common_words)\n",
    "\n",
    "\n",
    "# Adding select words to stop words for better analysis on important word frequency\n",
    "stop = set(stopwords.words('english')) \n",
    "stop_words = [\"via\", \"co\", \"I\",'We','The'] + list(stop)\n",
    "\n",
    "# Removing stop words from the tweets\n",
    "df_all['word'] = df_all['word_cloud'].apply(lambda x: ' '.join([item for item in x.split() if item not in stop_words]))\n",
    "df_all['word'] = df_all['word'].str.replace(r'[^\\w\\s]+', '')\n",
    "\n",
    "# Separating the strings to a list of words\n",
    "word_list = [word for line in df_all['word'] for word in line.split()]\n",
    "\n",
    "# Creating a word frequency counter\n",
    "sns.set(style=\"darkgrid\")\n",
    "counts = Counter(word_list).most_common(15)\n",
    "counts_df = pd.DataFrame(counts)\n",
    "counts_df\n",
    "counts_df.columns = ['word', 'frequency']\n",
    "\n",
    "# Creating a word frequency plot\n",
    "fig, ax = plt.subplots(figsize = (9, 9))\n",
    "ax = sns.barplot(y=\"word\", x='frequency', ax = ax, data=counts_df, palette=\"hls\")\n",
    "plt.title('WORD FREQUENCY')\n",
    "plt.savefig('wordcount_bar.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above bar graph we have removed the most common words we have seen which were appearing most often from the wordclouds words like 'climate change', 'Donald Trump'... \n",
    "since we have done the analysis for them and stated possible reasons why they might be appearing more often so we want to see which other words are used when we excluded them.\n",
    "\n",
    "\n",
    "As we can see that **Believe** is the top word with a frequency above 1000 times which might from here we can't clearly say which group likes using this word but we know it is the most used word in our dataset when we excluded some of the common words we have seen and **real** take the second position in our list which might be a sign that most people are saying climate change is a real thing. And the top 15 words when you remove the obvious ones which were most appearing in most tweets are: Believe, Real, World, Environmental Protection Agency(EPA), People, U as YOU, Make, Fight, Say, Die, Like, New, Scientist, Cause, Go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Mentions Analysis\n",
    "#### Insights on the mentions\n",
    "#### Bar graph representation of the mention insights \n",
    "The numbers below about the mentions convey the same thing as the cluster bar graph below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:36.828415Z",
     "iopub.status.busy": "2021-06-22T13:51:36.828138Z",
     "iopub.status.idle": "2021-06-22T13:51:36.846578Z",
     "shell.execute_reply": "2021-06-22T13:51:36.845376Z",
     "shell.execute_reply.started": "2021-06-22T13:51:36.828387Z"
    }
   },
   "outputs": [],
   "source": [
    "mentions =['All', 'Postive', 'Neutral', 'Negative', 'News']\n",
    "\n",
    "fig = go.Figure(data=[\n",
    "    go.Bar(name='Total Mentions', x=mentions, y=[14799, 8497, 2198, 1386, 2718],marker_color='lightblue'),\n",
    "    go.Bar(name='Unique Mentions', x=mentions, y=[7640, 4495, 1880, 919, 1302], marker_color ='purple')\n",
    "])\n",
    "# Change the bar mode\n",
    "fig.update_layout(barmode='group', title = \"Distribution of Mentions\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:36.848900Z",
     "iopub.status.busy": "2021-06-22T13:51:36.848277Z",
     "iopub.status.idle": "2021-06-22T13:51:36.867510Z",
     "shell.execute_reply": "2021-06-22T13:51:36.866484Z",
     "shell.execute_reply.started": "2021-06-22T13:51:36.848854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get count of total mentions and  total unique mentions\n",
    "print(\"Total number of mentions: \\t\\t\\t\"+ str(len(mentions_list)))\n",
    "print(\"Total number of unique mentions: \\t\\t\"+ str(len(set(mentions_list))))\n",
    "\n",
    "# Get count of mentions and unique mentions per sentiment\n",
    "print(\"Total number of News mentions: \\t\\t\\t\"+ str(len(news_mentions)))\n",
    "print(\"Total number of unique News mentions: \\t\\t\"+ str(len(set(news_mentions))))\n",
    "\n",
    "print(\"Total number of Positve mentions: \\t\\t\"+ str(len(pos_mentions)))\n",
    "print(\"Total number of unique Positive mentions: \\t\"+ str(len(set(pos_mentions))))\n",
    "\n",
    "print(\"Total number of Neutral mentions: \\t\\t\"+ str(len(neutral_mentions)))\n",
    "print(\"Total number of unique Neutral mentions: \\t\"+ str(len(set(neutral_mentions))))\n",
    "\n",
    "print(\"Total number of Negative mentions: \\t\\t\"+ str(len(neg_mentions)))\n",
    "print(\"Total number of unique Negative mentions: \\t\"+ str(len(set(neg_mentions))))\n",
    "\n",
    "# Count of common mentions\n",
    "common_mentions = set(pos_mentions) & set(news_mentions) & set(neg_mentions) & set(neutral_mentions)\n",
    "print(\"Total number of Common mentions: \\t\\t\"+ str(len(common_mentions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that we can tell which sentiment had more mentions and which has most unique mentions so far but we not able to see what the actual mentions were, so for us to visualise that we going to use a word cloud to see the actual mentions for each sentiment ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wordclouds representations of the mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:51:36.869227Z",
     "iopub.status.busy": "2021-06-22T13:51:36.868919Z",
     "iopub.status.idle": "2021-06-22T13:52:26.552989Z",
     "shell.execute_reply": "2021-06-22T13:52:26.551948Z",
     "shell.execute_reply.started": "2021-06-22T13:51:36.869197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extracing rows per sentiment\n",
    "news = df_all[df_all['sentiment'] == 'News']['mentions']\n",
    "pos = df_all[df_all['sentiment'] == 'Pro']['mentions']\n",
    "neutral = df_all[df_all['sentiment'] == 'Neutral']['mentions']\n",
    "neg = df_all[df_all['sentiment'] =='Anti']['mentions']\n",
    "\n",
    "# Splitting strings into lists\n",
    "news = [word for line in news for word in line]\n",
    "pos = [word for line in pos for word in line]\n",
    "neutral = [word for line in neutral for word in line]\n",
    "neg = [word for line in neg for word in line]\n",
    "\n",
    "# Wordcloud for the News mentions \n",
    "news = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=100,\n",
    "    max_font_size=60, \n",
    "    scale=20,\n",
    "    random_state=42,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(news))\n",
    "\n",
    "# Wordcloud for the Pro mentions\n",
    "pos = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=100,\n",
    "    max_font_size=60, \n",
    "    scale=20,\n",
    "    random_state=42,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(pos))\n",
    "\n",
    "# Wordcloud for the Neutral mentions\n",
    "neutral = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=100,\n",
    "    max_font_size=60, \n",
    "    scale=20,\n",
    "    random_state=42,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(neutral))\n",
    "\n",
    "# Wordcloud for the Anti mentions\n",
    "neg = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=100,\n",
    "    max_font_size=60, \n",
    "    scale=20,\n",
    "    random_state=42,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(neg))\n",
    "\n",
    "##Creating individual wordclouds for each sentiments title\n",
    "fig, axs = plt.subplots(2, 2, figsize = (20, 12))\n",
    "fig.tight_layout(pad = 0)\n",
    "\n",
    "axs[0, 0].imshow(news)\n",
    "axs[0, 0].set_title('Common News mentions', fontsize = 20)\n",
    "axs[0, 0].axis('off')\n",
    "\n",
    "axs[0, 1].imshow(pos)\n",
    "axs[0, 1].set_title('Common Positive mentions', fontsize = 20)\n",
    "axs[0, 1].axis('off')\n",
    "\n",
    "axs[1, 0].imshow(neg)\n",
    "axs[1, 0].set_title('Common Negative mentions ', fontsize = 20)\n",
    "axs[1, 0].axis('off')\n",
    "\n",
    "axs[1, 1].imshow(neutral)\n",
    "axs[1, 1].set_title('Common Neutral mentions ', fontsize = 20)\n",
    "axs[1, 1].axis('off')\n",
    "\n",
    "plt.savefig('joint_cloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the wordclouds above about the mentions we see the following:\n",
    "\n",
    "_**Frequent mentions under News climate change:**_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_**Frequent mentions under  climate change:**_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_**Frequent mentions under Neutral climate change:**_\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "_**Frequent mentions under Anti climate change:**_\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram Analyis per Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:26.554688Z",
     "iopub.status.busy": "2021-06-22T13:52:26.554370Z",
     "iopub.status.idle": "2021-06-22T13:52:29.334628Z",
     "shell.execute_reply": "2021-06-22T13:52:29.333482Z",
     "shell.execute_reply.started": "2021-06-22T13:52:26.554657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make a set of stop words\n",
    "fig.suptitle('Bigrams in Tweets')\n",
    "stopwords = set(STOPWORDS)\n",
    "more_stopwords = {'https','https rt'}\n",
    "stopwords = stopwords.union(more_stopwords)\n",
    "\n",
    "# Plot for the Pro sentiment of the bigrams\n",
    "plt.figure(figsize=(16,12))\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "bigram_d = list(\n",
    "    bigrams(\n",
    "        [w for w in word_tokenize(' '.join(df_all.loc[df_all.sentiment=='Pro', 'word']).lower()) \n",
    "        if (w not in stopwords) & (w.isalpha())]\n",
    "    )\n",
    ")\n",
    "\n",
    "d_fq = FreqDist(bg for bg in bigram_d)\n",
    "bgdf_d = pd.DataFrame.from_dict(d_fq, orient='index', columns=['count'])\n",
    "bgdf_d.index = bgdf_d.index.map(lambda x: ' '.join(x))\n",
    "bgdf_d = bgdf_d.sort_values('count',ascending=False)\n",
    "sns.barplot(bgdf_d.head(10)['count'], bgdf_d.index[:10], color='pink')\n",
    "plt.title('Pro Tweets')\n",
    "\n",
    "# Plot for the News sentiment of the bigrams\n",
    "plt.subplot(2,2,2)\n",
    "bigram_nd = list(bigrams([w for w in word_tokenize(' '.join(df_all.loc[df_all.sentiment=='News', 'word']).lower()) if \n",
    "              (w not in stopwords) & (w.isalpha())]))\n",
    "nd_fq = FreqDist(bg for bg in bigram_nd)\n",
    "bgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\n",
    "bgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\n",
    "bgdf_nd = bgdf_nd.sort_values('count',ascending=False)\n",
    "sns.barplot(bgdf_nd.head(10)['count'], bgdf_nd.index[:10], color='b')\n",
    "plt.title('News Tweets')\n",
    "\n",
    "# Plot for the Anti sentiment of the bigrams\n",
    "plt.subplot(2,2,3)\n",
    "bigram_nd = list(bigrams([w for w in word_tokenize(' '.join(df_all.loc[df_all.sentiment=='Anti', 'word']).lower()) if \n",
    "              (w not in stopwords) & (w.isalpha())]))\n",
    "nd_fq = FreqDist(bg for bg in bigram_nd)\n",
    "bgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\n",
    "bgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\n",
    "bgdf_nd = bgdf_nd.sort_values('count',ascending=False)\n",
    "sns.barplot(bgdf_nd.head(10)['count'], bgdf_nd.index[:10], color='c')\n",
    "plt.title('Anti Tweets')\n",
    "\n",
    "# Plot for the Neutral sentiment of the bigrams\n",
    "plt.subplot(2,2,4)\n",
    "bigram_nd = list(bigrams([w for w in word_tokenize(' '.join(df_all.loc[df_all.sentiment=='Neutral', 'word']).lower()) if \n",
    "              (w not in stopwords) & (w.isalpha())]))\n",
    "nd_fq = FreqDist(bg for bg in bigram_nd)\n",
    "bgdf_nd = pd.DataFrame.from_dict(nd_fq, orient='index', columns=['count'])\n",
    "bgdf_nd.index = bgdf_nd.index.map(lambda x: ' '.join(x))\n",
    "bgdf_nd = bgdf_nd.sort_values('count',ascending=False)\n",
    "sns.barplot(bgdf_nd.head(10)['count'], bgdf_nd.index[:10], color='g')\n",
    "plt.title('Neutral Tweets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:29.336324Z",
     "iopub.status.busy": "2021-06-22T13:52:29.335995Z",
     "iopub.status.idle": "2021-06-22T13:52:29.346358Z",
     "shell.execute_reply": "2021-06-22T13:52:29.345253Z",
     "shell.execute_reply.started": "2021-06-22T13:52:29.336291Z"
    }
   },
   "outputs": [],
   "source": [
    "#Splitting features and target variables\n",
    "X = df_all['message']#X is the features of the cleaned tweets\n",
    "\n",
    "y = df_all['sentiment'] #Y is the target variable which is the train sentiment\n",
    "\n",
    "# Splitting the data 90% for training and 10% for testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Text feature extraction\n",
    "​\n",
    "Text feature extraction is the process of transforming what is essentially a list of words into a feature set that is usable by a classifier. The NLTK classifiers expect dict style feature sets, so we must therefore transform our text into a dict.\n",
    "​\n",
    "The TFIDFVectorizer (The term frequency-inverse document frequency) is a weight whose value increases proportionally to count, but is inversely proportional to frequency of the word in the corpus.The Tf-idf can be successfully used for stop-words filtering in various subject fields including text summarization and classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:29.348189Z",
     "iopub.status.busy": "2021-06-22T13:52:29.347883Z",
     "iopub.status.idle": "2021-06-22T13:52:29.361881Z",
     "shell.execute_reply": "2021-06-22T13:52:29.360900Z",
     "shell.execute_reply.started": "2021-06-22T13:52:29.348159Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer() #Call the TFidfVectorizer\n",
    "\n",
    "cf= CountVectorizer() #Call the CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1 Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/725/1*QY3CSyA4BzAU6sEPFwp9ZQ.png\">\n",
    "\n",
    "Logistic regression is a classification algorithm used to assign observations to a discrete set of classes. Some of the examples of classification problems are Email spam or not spam, Online transactions Fraud or not Fraud, Tumor Malignant or Benign. Logistic regression transforms its output using the logistic sigmoid function to return a probability value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:29.363407Z",
     "iopub.status.busy": "2021-06-22T13:52:29.363121Z",
     "iopub.status.idle": "2021-06-22T13:52:40.714896Z",
     "shell.execute_reply": "2021-06-22T13:52:40.713877Z",
     "shell.execute_reply.started": "2021-06-22T13:52:29.363379Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(C=1, class_weight='balanced', max_iter=1000) # initilising the model\n",
    "  \n",
    "clf_lr = Pipeline([('tfidf', tfidf), ('clf', lr)]) # Create a pipeline\n",
    "\n",
    "clf_lr.fit(X_train, y_train) # Fit the training data to the pipeline\n",
    "\n",
    "y_pred_lr= clf_lr.predict(X_test) # Make predictions\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred_lr, y_test)) # Print the accuracy\n",
    "\n",
    "print('f1_score %s' % metrics.f1_score(y_test,y_pred_lr,average='weighted')) # Print the weighted f1 score\n",
    "\n",
    "print(classification_report(y_test, y_pred_lr)) # Print out the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 Linear SVC classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://data-flair.training/blogs/wp-content/uploads/sites/2/2019/07/introduction-to-SVM.png\">\n",
    "\n",
    "SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:40.719866Z",
     "iopub.status.busy": "2021-06-22T13:52:40.719517Z",
     "iopub.status.idle": "2021-06-22T13:52:41.584874Z",
     "shell.execute_reply": "2021-06-22T13:52:41.583815Z",
     "shell.execute_reply.started": "2021-06-22T13:52:40.719831Z"
    }
   },
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(random_state=42) # initilising the model\n",
    "\n",
    "clf_lsvc = Pipeline([('tfidf', tfidf), ('clf', lsvc)]) # Create a pipeline\n",
    "\n",
    "clf_lsvc.fit(X_train, y_train) # Fit the training data to the pipeline\n",
    "\n",
    "y_pred_lsvc = clf_lsvc.predict(X_test) # Making the predictions\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred_lsvc, y_test)) # Print the accuracy\n",
    "\n",
    "print('f1_score %s' % metrics.f1_score(y_test,y_pred_lsvc,average='weighted')) # Print the f1 score\n",
    "\n",
    "print(classification_report(y_test, y_pred_lsvc)) # Print out the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3  K-NN Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/3544/1*mAgqYN_HLbYYXXkQdyBA6Q.png\">\n",
    "\n",
    "The k-nearest neighbors (KNN) algorithm is a simple, supervised machine learning algorithm that can be used to solve both classification and regression problems. It’s easy to implement and understand, but has a major drawback of becoming significantly slows as the size of that data in use grows.\n",
    "KNN works by finding the distances between a query and all the examples in the data, selecting the specified number examples (K) closest to the query, then votes for the most frequent label (in the case of classification) or averages the labels (in the case of regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:41.588897Z",
     "iopub.status.busy": "2021-06-22T13:52:41.588543Z",
     "iopub.status.idle": "2021-06-22T13:52:43.454662Z",
     "shell.execute_reply": "2021-06-22T13:52:43.453095Z",
     "shell.execute_reply.started": "2021-06-22T13:52:41.588863Z"
    }
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2) # initilising the model\n",
    "\n",
    "clf_knn = Pipeline([('tfidf', tfidf), ('clf', knn)]) # Create a pipeline\n",
    "\n",
    "clf_knn.fit(X_train, y_train) # Fit the training data to the pipeline\n",
    "\n",
    "y_pred_knn = clf_knn.predict(X_test) # Making the predictions\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred_knn, y_test)) # Print the accuracy\n",
    "\n",
    "print('f1_score %s' % metrics.f1_score(y_test,y_pred_knn,average='weighted')) # Print the f1 score\n",
    "\n",
    "print(classification_report(y_test, y_pred_knn)) # Print out the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.4 Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.kdnuggets.com/wp-content/uploads/bayes-nagesh-1.jpg\">\n",
    "\n",
    "Naïve Bayes algorithm is a supervised learning algorithm, which is based on Bayes theorem and used for solving classification problems.\n",
    "It is mainly used in text classification that includes a high-dimensional training dataset.\n",
    "Naïve Bayes Classifier is one of the simple and most effective Classification algorithms which helps in building the fast machine learning models that can make quick predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:43.456557Z",
     "iopub.status.busy": "2021-06-22T13:52:43.456202Z",
     "iopub.status.idle": "2021-06-22T13:52:44.084126Z",
     "shell.execute_reply": "2021-06-22T13:52:44.082940Z",
     "shell.execute_reply.started": "2021-06-22T13:52:43.456524Z"
    }
   },
   "outputs": [],
   "source": [
    "nb = MultinomialNB() # initilising the model\n",
    "\n",
    "clf_nb= Pipeline([('tfidf', tfidf), ('clf', nb)]) # Create a pipeline\n",
    "\n",
    "clf_nb.fit(X_train, y_train) # Fit the training data to the pipeline\n",
    "\n",
    "y_pred_nb = clf_nb.predict(X_test) # Making the predictions\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred_nb, y_test)) # Print the accuracy\n",
    "\n",
    "print('f1_score %s' % metrics.f1_score(y_test,y_pred_nb,average='weighted')) # Print the f1 score\n",
    "\n",
    "print(classification_report(y_test, y_pred_nb)) # Print out the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.5 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.analyticsvidhya.com/wp-content/uploads/2020/02/rfc_vs_dt1.png\">\n",
    "\n",
    "The general idea of the Random Forest model is that a combination of machine learning models increase the overall result. Put simply: random forest builds multiple decision trees and merges them together to get a more accurate and stable prediction (e.g in the image above).\n",
    "\n",
    "Lets see what results we get from the Random Forest Classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:44.085942Z",
     "iopub.status.busy": "2021-06-22T13:52:44.085574Z",
     "iopub.status.idle": "2021-06-22T13:52:45.192445Z",
     "shell.execute_reply": "2021-06-22T13:52:45.191385Z",
     "shell.execute_reply.started": "2021-06-22T13:52:44.085907Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth=5, n_estimators=100)\n",
    "\n",
    "clf_rf = Pipeline([('tfidf', tfidf), ('clf', rf)])  # Create a pipeline\n",
    "\n",
    "clf_rf.fit(X_train, y_train) # Fit the training data to the pipeline\n",
    "\n",
    "y_pred_rf = clf_rf.predict(X_test) # Making the predictions\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred_rf, y_test)) #Print the accuracy\n",
    "\n",
    "print('f1_score %s' % metrics.f1_score(y_test,y_pred_rf,average='weighted')) #Print the f1 score\n",
    "\n",
    "print(classification_report(y_test, y_pred_rf)) #Print out the classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing the sentiments\n",
    "#### Class size to up/down sample\n",
    "\n",
    "Resampling is a process which involves changing the frequency of the observations. Below we apply it to assess whether or not it will improve our training. This process also serves as model tuning as we will be using a define resampling strategy while taking note of performance measure.\n",
    "\n",
    "\n",
    "**We will be using a copy for resampling, so we can see what results we will find when we train our model on a balanced dataset instead.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:45.194530Z",
     "iopub.status.busy": "2021-06-22T13:52:45.193938Z",
     "iopub.status.idle": "2021-06-22T13:52:45.207116Z",
     "shell.execute_reply": "2021-06-22T13:52:45.206061Z",
     "shell.execute_reply.started": "2021-06-22T13:52:45.194483Z"
    }
   },
   "outputs": [],
   "source": [
    "data = df_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:45.208852Z",
     "iopub.status.busy": "2021-06-22T13:52:45.208387Z",
     "iopub.status.idle": "2021-06-22T13:52:45.229832Z",
     "shell.execute_reply": "2021-06-22T13:52:45.228483Z",
     "shell.execute_reply.started": "2021-06-22T13:52:45.208789Z"
    }
   },
   "outputs": [],
   "source": [
    "# importing the module and creating a resampling variable\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Determining the class size using the class with more observations\n",
    "class_size = int(len(data[data['sentiment']=='Pro'])/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pro class is the one we will use to resample others since it is the one with most obseravtions or tweets in our data, meaning the rest will be upsampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:45.231953Z",
     "iopub.status.busy": "2021-06-22T13:52:45.231569Z",
     "iopub.status.idle": "2021-06-22T13:52:45.260027Z",
     "shell.execute_reply": "2021-06-22T13:52:45.258914Z",
     "shell.execute_reply.started": "2021-06-22T13:52:45.231918Z"
    }
   },
   "outputs": [],
   "source": [
    "# seperating the four classes so we can later be able to know by how much we need to add in each group\n",
    "class_1 = data[data['sentiment']=='Anti'] \n",
    "\n",
    "class_2 = data[data['sentiment']=='Neutral']\n",
    "\n",
    "class_3 = data[data['sentiment']=='Pro']\n",
    "\n",
    "class_4 = data[data['sentiment']=='News']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Up-sampling\n",
    "\n",
    "Here we increase the frequency of the samples which are below the Pro group and keeping the Pro group constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:45.262160Z",
     "iopub.status.busy": "2021-06-22T13:52:45.261565Z",
     "iopub.status.idle": "2021-06-22T13:52:45.283937Z",
     "shell.execute_reply": "2021-06-22T13:52:45.282896Z",
     "shell.execute_reply.started": "2021-06-22T13:52:45.262107Z"
    }
   },
   "outputs": [],
   "source": [
    "# upsampling classes 1, 2, and 4 & downsampling class 3\n",
    "class_1_up = resample(class_1,replace=True,n_samples=class_size, random_state=27)\n",
    "\n",
    "class_2_up = resample(class_2,replace=True,n_samples=class_size, random_state=27)\n",
    "\n",
    "class_4_up = resample(class_4,replace=True,n_samples=class_size, random_state=27)\n",
    "\n",
    "class_3_down = resample(class_3,replace=False,n_samples=class_size, random_state=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:45.285542Z",
     "iopub.status.busy": "2021-06-22T13:52:45.285246Z",
     "iopub.status.idle": "2021-06-22T13:52:45.383838Z",
     "shell.execute_reply": "2021-06-22T13:52:45.382861Z",
     "shell.execute_reply.started": "2021-06-22T13:52:45.285514Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a new DataFrame out of the balanced bata so we can easily work with it.\n",
    "res_df = pd.concat([class_1_up, class_2_up, class_4_up,class_3_down])\n",
    "\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:45.385454Z",
     "iopub.status.busy": "2021-06-22T13:52:45.385153Z",
     "iopub.status.idle": "2021-06-22T13:52:45.584615Z",
     "shell.execute_reply": "2021-06-22T13:52:45.583533Z",
     "shell.execute_reply.started": "2021-06-22T13:52:45.385424Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking if data has been well-balanced\n",
    "sns.countplot(x = res_df['sentiment'], data = data, palette='PRGn')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equal distribution of the sentiments as shown on the count plot, confirms that sentiments are now balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:45.586346Z",
     "iopub.status.busy": "2021-06-22T13:52:45.586035Z",
     "iopub.status.idle": "2021-06-22T13:52:48.251956Z",
     "shell.execute_reply": "2021-06-22T13:52:48.251093Z",
     "shell.execute_reply.started": "2021-06-22T13:52:45.586318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining the features as well as the label\n",
    "X1 = res_df['message']\n",
    "\n",
    "# data cleaning\n",
    "X_res= X1.apply(clean_text) \n",
    "\n",
    "X_res = X1.apply(strip_accents)\n",
    "\n",
    "X_res= X1.apply(lambda x: ''.join([l for l in x if l not in string.punctuation]))\n",
    "\n",
    "\n",
    "y_res = res_df['sentiment']\n",
    "\n",
    "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X_res, y_res, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Logistic regression on the balanced data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:48.253477Z",
     "iopub.status.busy": "2021-06-22T13:52:48.253023Z",
     "iopub.status.idle": "2021-06-22T13:52:53.705642Z",
     "shell.execute_reply": "2021-06-22T13:52:53.704424Z",
     "shell.execute_reply.started": "2021-06-22T13:52:48.253446Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_B = LogisticRegression(C=1, class_weight='balanced', max_iter=1000) # initilising the model\n",
    "\n",
    "clf_lr_B = Pipeline([('tfidf', tfidf), ('clf', lr_B)]) # Create a pipeline\n",
    "\n",
    "clf_lr_B.fit(X_trainB, y_trainB) # Fit the training data to the pipeline\n",
    "\n",
    "y_pred_lr_B = clf_lr_B.predict(X_testB) # Make predictions\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred_lr_B, y_testB)) # Print the accuracy\n",
    "\n",
    "print('f1_score %s' % metrics.f1_score(y_testB,y_pred_lr_B,average='weighted')) # Print the weighted f1 score\n",
    "\n",
    "print(classification_report(y_testB, y_pred_lr_B)) # Print out the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Linear SVC classification on the balanced data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:53.707304Z",
     "iopub.status.busy": "2021-06-22T13:52:53.706983Z",
     "iopub.status.idle": "2021-06-22T13:52:54.633848Z",
     "shell.execute_reply": "2021-06-22T13:52:54.632855Z",
     "shell.execute_reply.started": "2021-06-22T13:52:53.707273Z"
    }
   },
   "outputs": [],
   "source": [
    "lsvc_B = LinearSVC(random_state=42) # initilising the model\n",
    "\n",
    "clf_lsvc_B = Pipeline([('tfidf', tfidf), ('clf', lsvc_B)]) # Create a pipeline\n",
    "\n",
    "clf_lsvc_B.fit(X_trainB, y_trainB) # fitting the training dataset\n",
    "\n",
    "y_pred_lsvc_B = clf_lsvc_B.predict(X_testB) # making the predictions\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred_lsvc_B, y_testB)) # Print the accuracy\n",
    "\n",
    "print('f1_score %s' % metrics.f1_score(y_testB,y_pred_lsvc_B,average='weighted')) # Print the f1 score\n",
    "\n",
    "print(classification_report(y_testB, y_pred_lsvc_B)) # Print out the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-NN Classification on the balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:54.635364Z",
     "iopub.status.busy": "2021-06-22T13:52:54.635063Z",
     "iopub.status.idle": "2021-06-22T13:52:58.196564Z",
     "shell.execute_reply": "2021-06-22T13:52:58.195356Z",
     "shell.execute_reply.started": "2021-06-22T13:52:54.635334Z"
    }
   },
   "outputs": [],
   "source": [
    "knn_B = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2) # initilising the model\n",
    "\n",
    "clf_knn_B = Pipeline([('tfidf', tfidf), ('clf', knn_B)]) # Create a pipeline\n",
    "\n",
    "clf_knn_B.fit(X_trainB, y_trainB) # fitting the training dataset\n",
    "\n",
    "y_pred_knn_B = clf_knn_B.predict(X_testB) # making the predictions\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred_knn_B, y_testB)) # Print the accuracy\n",
    "\n",
    "print('f1_score %s' % metrics.f1_score(y_testB,y_pred_knn_B,average='weighted')) # Print the f1 score\n",
    "\n",
    "print(classification_report(y_testB, y_pred_knn_B)) # Print out the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Naïve Bayes on the balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:58.198718Z",
     "iopub.status.busy": "2021-06-22T13:52:58.198289Z",
     "iopub.status.idle": "2021-06-22T13:52:58.874457Z",
     "shell.execute_reply": "2021-06-22T13:52:58.873214Z",
     "shell.execute_reply.started": "2021-06-22T13:52:58.198664Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_B = MultinomialNB() # initilising the model\n",
    "\n",
    "clf_nb_B= Pipeline([('tfidf', tfidf), ('clf', nb_B)]) # Create a pipeline\n",
    "\n",
    "clf_nb_B.fit(X_trainB, y_trainB) # fitting the training dataset\n",
    "\n",
    "y_pred_nb_B = clf_nb_B.predict(X_testB) # making the predictions\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred_nb_B, y_testB)) # Print the accuracy\n",
    "\n",
    "print('f1_score %s' % metrics.f1_score(y_testB,y_pred_nb_B,average='weighted')) # Print the f1 score\n",
    "\n",
    "print(classification_report(y_testB, y_pred_nb_B)) # Print out the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier on the balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:52:58.876470Z",
     "iopub.status.busy": "2021-06-22T13:52:58.876015Z",
     "iopub.status.idle": "2021-06-22T13:53:00.103254Z",
     "shell.execute_reply": "2021-06-22T13:53:00.102203Z",
     "shell.execute_reply.started": "2021-06-22T13:52:58.876424Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_B = RandomForestClassifier(max_depth=5, n_estimators=100) # initilising the model\n",
    "\n",
    "clf_rf_B = Pipeline([('tfidf', tfidf), ('clf', rf_B)]) # Create a pipeline\n",
    "\n",
    "clf_rf_B.fit(X_trainB, y_trainB) # fitting the training dataset\n",
    "\n",
    "y_pred_rf_B = clf_rf_B.predict(X_testB) # making the predictions\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred_rf_B, y_testB)) # Print the accuracy\n",
    "\n",
    "print('f1_score %s' % metrics.f1_score(y_testB,y_pred_rf_B,average='weighted')) # Print the f1 score\n",
    "\n",
    "print(classification_report(y_testB, y_pred_rf_B)) # Print out the classification report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tuning\"></a>\n",
    "# 6. Model Parameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. By contrast, the values of other parameters (typically node weights) are learned.\n",
    "\n",
    "The same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Tune the lsvc model using GridSearch\n",
    "\n",
    "A range of different optimization algorithms may be used. Grid Search Defines a search space as a grid of hyperparameter values and evaluates every position in the grid. Grid search is great for spot-checking combinations that are known to perform well generally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:53:00.104922Z",
     "iopub.status.busy": "2021-06-22T13:53:00.104570Z",
     "iopub.status.idle": "2021-06-22T13:53:00.111499Z",
     "shell.execute_reply": "2021-06-22T13:53:00.110562Z",
     "shell.execute_reply.started": "2021-06-22T13:53:00.104885Z"
    }
   },
   "outputs": [],
   "source": [
    "#We have commented out the next two cells because they take about 30 minutes to run\n",
    "'''\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(max_df=0.75, min_df=2, ngram_range=(1, 2))),\n",
    "    ('clf', LinearSVC(random_state=42))])\n",
    "\n",
    "parameters = {'clf':[LinearSVC(random_state = 42)],\n",
    "           'clf__penalty':['l1','l2'],\n",
    "           'clf__C':[0.1, 1, 10, 100],\n",
    "           'clf__multi_class':['ovr', 'crammer_singer'],\n",
    "           'clf__class_weight':['balanced',None]}\n",
    "\n",
    "grid_search_tune = GridSearchCV(pipeline, parameters, cv=5, n_jobs=-1, verbose=3)\n",
    "grid_search_tune.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters set:\")\n",
    "print(grid_search_tune.best_estimator_.steps)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:53:00.113078Z",
     "iopub.status.busy": "2021-06-22T13:53:00.112732Z",
     "iopub.status.idle": "2021-06-22T13:53:00.126087Z",
     "shell.execute_reply": "2021-06-22T13:53:00.124852Z",
     "shell.execute_reply.started": "2021-06-22T13:53:00.113046Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "y_pred_grid = grid_search_tune.predict(X_test)#predict\n",
    "\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred_grid, y_test)) #Print the accuracy\n",
    "\n",
    "print('f1_score %s' % metrics.f1_score(y_test,y_pred_grid,average='weighted')) #Print the f1 score\n",
    "\n",
    "print(classification_report(y_test, y_pred_grid)) #Print out the classification\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation\"></a>\n",
    "# 7. Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is not always the best metric to use when evaluating imbalanced datasets as it can be very misleading. Metrics that can provide better insight include:\n",
    "\n",
    "1. Confusion Matrix: table shows correct predictions and types of incorrect predictions.\n",
    "2. Precision: the number of true positives divided by all positive predictions. \n",
    "3. Recall: the number of true positives divided by the number of positive values in the test data. \n",
    "4. F1: Score: the weighted average of precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best performing models\n",
    "#### Linear SVC classifier for balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:53:00.128291Z",
     "iopub.status.busy": "2021-06-22T13:53:00.127841Z",
     "iopub.status.idle": "2021-06-22T13:53:00.349363Z",
     "shell.execute_reply": "2021-06-22T13:53:00.348306Z",
     "shell.execute_reply.started": "2021-06-22T13:53:00.128244Z"
    }
   },
   "outputs": [],
   "source": [
    "# column names\n",
    "new = list(set(res_df.sentiment.values))\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y_testB,y_pred_lsvc_B )\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix,columns=new,index=new)\n",
    "\n",
    "#Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(10, 10), facecolor='w', edgecolor='k')\n",
    "\n",
    "# font size\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# the actual heatmap\n",
    "sns.heatmap(conf_matrix_df,cmap='coolwarm',annot=True,fmt='.5g',cbar=False)\n",
    "\n",
    "# x axis label\n",
    "plt.xlabel('Predicted',fontsize=22)\n",
    "\n",
    "# y axis label\n",
    "plt.ylabel('Actual',fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear SVC classifier for imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-22T13:53:00.351531Z",
     "iopub.status.busy": "2021-06-22T13:53:00.350937Z",
     "iopub.status.idle": "2021-06-22T13:53:00.552182Z",
     "shell.execute_reply": "2021-06-22T13:53:00.551388Z",
     "shell.execute_reply.started": "2021-06-22T13:53:00.351485Z"
    }
   },
   "outputs": [],
   "source": [
    "# column names\n",
    "new = list(set(df_all.sentiment.values))\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test,y_pred_lsvc )\n",
    "conf_matrix_df = pd.DataFrame(conf_matrix,columns=new,index=new)\n",
    "\n",
    "#Plot confusion matrix heatmap\n",
    "plt.figure(figsize=(10, 10), facecolor='w', edgecolor='k')\n",
    "\n",
    "# font size\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# the actual heatmap\n",
    "sns.heatmap(conf_matrix_df,cmap='coolwarm',annot=True,fmt='.5g',cbar=False)\n",
    "\n",
    "# x axis label\n",
    "plt.xlabel('Predicted',fontsize=22)\n",
    "\n",
    "# y axis label\n",
    "plt.ylabel('Actual',fontsize=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "Linear SVC is able to successfully classify the tweets.\n",
    "This model classifies most tweets successfully with clear boundaries and less confusion surrounding the pro climate change class compared to the first 3 models.\n",
    "This model shows a higher degree of confusion surrounding the Neutral class. Since about 16 tweets from the Neutral group were classified as Pro and 8 were classified as Anti and 47 were News this is because people from the neutral group can easily be mistaken with all the groups since they normally tweet general things.\n",
    "This, however, leads to an increase in the precision, accuracy and f1 score for the pro class which makes up the majority of the tweets.\n",
    "Linear SVC has achieved the highest F1 score of 0.758"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "# 9. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than half of the tweets examined support the beilief of man-made climate change. Futhermore, climate change is now one of the two most important issues in politics for Democrats. The data also suggests that the majority of anti climate change tweets come from Republicans and Trump supporters.\n",
    "\n",
    "We noticed that the majority of tweets about climate change accross all classes involve the Paris agreement, COP22, Trump and Trump related hashtags/mentions. It was intresting to note that the most links are being shared in the News climate change class and not in any other related class, as we would have expected news to share more source of information to the public.\n",
    "\n",
    "Our final kaggle subission made use of a tuned linear SVC model and achived an F1 score of 0.75343\n",
    "\n",
    "For further information regarding the possible business applications of these insights and as well as access to our interactive classification model and data visualizations please visit our streamlit app:\n",
    "\n",
    "Link: http://52.49.254.129:5000/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"references\"></a>\n",
    "# 10. References\n",
    "\n",
    "1. Comet related information: https://www.comet.ml/docs/python-sdk/scikit/\n",
    "\n",
    "2. Scott morrison's comments: https://www.theguardian.com/business/grogonomics/2020/jan/22/scott-morrisons-stance-on-climate-change-makes-it-harder-for-future-governments-to-undo-his-damage\n",
    "\n",
    "3. Barack Obama's comments: https://unfccc.int/news/president-obama-climate-change-greatest-threat-to-future-generations\n",
    "\n",
    "4. Information about the AMP: https://supportingcommunities.org/latest-news/2021/5/18/survey-results-the-amp-on-climate-change-amp-household-energy\n",
    "\n",
    "5. Models : Explore Data Science academy trains and preprocessings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
